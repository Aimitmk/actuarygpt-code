{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d8246e",
   "metadata": {},
   "source": [
    "# 複数のLLMモデルをLangchainで切り替えて使用するサンプル\n",
    "\n",
    "このノートブックでは以下のモデルを切り替えて使用できます：\n",
    "- OpenAI: GPT-4o-mini, GPT-4o\n",
    "- Anthropic: Claude Opus 4 thinking\n",
    "- Google: Gemini, Gemini Pro\n",
    "- DeepSeek: DeepSeek R1 0528\n",
    "- Groq: Grok 3 mini reasoning(high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9599a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインストール\n",
    "!pip install langchain langchain_openai langchain_anthropic langchain_google_genai langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937ed771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# APIキーの設定\n",
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Google Colabシークレットからキーを取得\n",
    "# あらかじめ「シークレット」タブで各APIキーを登録しておく必要があります\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('ANTHROPIC_API_KEY')\n",
    "os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
    "os.environ[\"GROQ_API_KEY\"] = userdata.get('GROQ_API_KEY')\n",
    "\n",
    "# 直接入力する場合（非推奨）\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578028c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なインポート\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dcaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DeepSeekのAPI呼び出し用の関数（Langchainでは現在直接サポートされていないため）\n",
    "def call_deepseek_api(prompt, model=\"deepseek-chat-r1-0528\"):\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {os.environ['DEEPSEEK_API_KEY']}\"\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        \"https://api.deepseek.ai/v1/chat/completions\",\n",
    "        json=data,\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c35c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各モデルを取得する関数\n",
    "def get_model(model_name):\n",
    "    \"\"\"\n",
    "    指定されたモデル名に基づいてLangChainモデルインスタンスを返す\n",
    "    \"\"\"\n",
    "    if model_name == \"gpt-4o-mini\":\n",
    "        return ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "    \n",
    "    elif model_name == \"gpt-4o\":\n",
    "        return ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "        \n",
    "    elif model_name == \"claude-opus-4-thinking\":\n",
    "        return ChatAnthropic(model=\"claude-3-opus-20240229\", temperature=0.7)\n",
    "        \n",
    "    elif model_name == \"gemini\":\n",
    "        return ChatGoogleGenerativeAI(model=\"gemini-1.0-pro\", temperature=0.7)\n",
    "        \n",
    "    elif model_name == \"gemini-pro\":\n",
    "        return ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0.7)\n",
    "        \n",
    "    elif model_name == \"grok-3-mini-reasoning\":\n",
    "        return ChatGroq(model=\"grok-3-mini\", temperature=0.7)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb68446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# チャットを実行する関数\n",
    "def chat_with_model(model_name, prompt, system_message=None):\n",
    "    \"\"\"\n",
    "    指定されたモデルでチャットを実行する\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): 使用するモデル名\n",
    "        prompt (str): プロンプト\n",
    "        system_message (str): システムメッセージ（オプション）\n",
    "    \"\"\"\n",
    "    # DeepSeek R1 0528は別処理\n",
    "    if model_name == \"deepseek-r1-0528\":\n",
    "        return call_deepseek_api(prompt)\n",
    "    \n",
    "    model = get_model(model_name)\n",
    "    messages = []\n",
    "    \n",
    "    # システムメッセージの設定（指定がある場合）\n",
    "    if system_message:\n",
    "        if model_name == \"claude-opus-4-thinking\":\n",
    "            messages.append(SystemMessage(content=system_message + \"\\nUse thinking out loud to reason through this problem step by step.\"))\n",
    "        elif model_name == \"grok-3-mini-reasoning\":\n",
    "            messages.append(SystemMessage(content=system_message + \"\\nUse step by step reasoning to solve this problem.\"))\n",
    "        else:\n",
    "            messages.append(SystemMessage(content=system_message))\n",
    "    elif model_name == \"claude-opus-4-thinking\":\n",
    "        messages.append(SystemMessage(content=\"Use thinking out loud to reason through this problem step by step.\"))\n",
    "    elif model_name == \"grok-3-mini-reasoning\":\n",
    "        messages.append(SystemMessage(content=\"Use step by step reasoning to solve this problem.\"))\n",
    "    \n",
    "    # ユーザーメッセージの追加\n",
    "    messages.append(HumanMessage(content=prompt))\n",
    "    \n",
    "    # モデルの呼び出し\n",
    "    response = model.invoke(messages)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578ca512",
   "metadata": {},
   "source": [
    "## モデルのテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3f3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用可能なモデル一覧\n",
    "models = [\n",
    "    \"gpt-4o-mini\",\n",
    "    \"gpt-4o\",\n",
    "    \"claude-opus-4-thinking\", \n",
    "    \"gemini\",\n",
    "    \"gemini-pro\",\n",
    "    \"deepseek-r1-0528\",\n",
    "    \"grok-3-mini-reasoning\"\n",
    "]\n",
    "\n",
    "# モデルを選択するためのドロップダウンを作成\n",
    "from ipywidgets import interact, Dropdown\n",
    "\n",
    "@interact(model_name=Dropdown(options=models, value=\"gpt-4o-mini\"))\n",
    "def test_model(model_name):\n",
    "    print(f\"選択されたモデル: {model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c855bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選んだモデルで質問を試す\n",
    "def ask_question(model_name, prompt, system_message=None):\n",
    "    print(f\"モデル {model_name} に質問中...\")\n",
    "    try:\n",
    "        response = chat_with_model(model_name, prompt, system_message)\n",
    "        print(\"\\n回答:\")\n",
    "        print(response)\n",
    "    except Exception as e:\n",
    "        print(f\"エラーが発生しました: {str(e)}\")\n",
    "        \n",
    "# 質問例\n",
    "sample_prompt = \"保険商品のプライシングについて、リスクベースのアプローチを簡単に説明してください。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選択したモデルで質問を試す\n",
    "model_name = \"gpt-4o-mini\"  # ここでモデル名を変更してテスト\n",
    "ask_question(model_name, sample_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdd4570",
   "metadata": {},
   "source": [
    "## インタラクティブなクエリ機能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5054b48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact, Text, Textarea\n",
    "\n",
    "@interact(model_name=Dropdown(options=models, value=\"gpt-4o-mini\"),\n",
    "          prompt=Textarea(value=sample_prompt, description=\"質問:\", layout={'width': '100%', 'height': '100px'}),\n",
    "          system_message=Text(value=\"\", description=\"システムメッセージ(オプション):\", layout={'width': '100%'}))\n",
    "def interactive_chat(model_name, prompt, system_message=\"\"):\n",
    "    if not prompt.strip():\n",
    "        print(\"質問を入力してください\")\n",
    "        return\n",
    "    \n",
    "    system_message = system_message if system_message.strip() else None\n",
    "    ask_question(model_name, prompt, system_message)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
